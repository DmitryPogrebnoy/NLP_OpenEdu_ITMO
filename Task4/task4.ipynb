{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "TextProc_Task_4_Students_en.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLYpSPDlW9LC"
   },
   "source": [
    "## Download everything we need\n",
    "\n",
    "Ne need to download WordNet by means of NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "YhTQ6EFXZ5R9",
    "outputId": "b9e976e9-2692-457d-b0ff-866869787d67"
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pogre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIQ4rEytvwLZ"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We import the data from a prepared text file. The file contains the set of word pairs (just nouns), for which expert similarity estimates are known.\n",
    "\n",
    "We make an associative array of \"word pair - similarity estimate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "           word_1       word_2  Score\n0       professor     cucumber   0.31\n1            monk        slave   0.92\n2      psychology   discipline   5.58\n3            life        death   7.88\n4    announcement   production   3.38\n..            ...          ...    ...\n145        energy    secretary   1.81\n146        planet         moon   8.08\n147       lobster         food   7.81\n148     precedent  information   3.85\n149     benchmark        index   4.25\n\n[150 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_1</th>\n      <th>word_2</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>professor</td>\n      <td>cucumber</td>\n      <td>0.31</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>monk</td>\n      <td>slave</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>psychology</td>\n      <td>discipline</td>\n      <td>5.58</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>life</td>\n      <td>death</td>\n      <td>7.88</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>announcement</td>\n      <td>production</td>\n      <td>3.38</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>energy</td>\n      <td>secretary</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>planet</td>\n      <td>moon</td>\n      <td>8.08</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>lobster</td>\n      <td>food</td>\n      <td>7.81</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>precedent</td>\n      <td>information</td>\n      <td>3.85</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>benchmark</td>\n      <td>index</td>\n      <td>4.25</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Task_4_sample_4.csv\")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{('professor', 'cucumber'): 0.31,\n ('monk', 'slave'): 0.92,\n ('psychology', 'discipline'): 5.58,\n ('life', 'death'): 7.88,\n ('announcement', 'production'): 3.38,\n ('word', 'similarity'): 4.75,\n ('drink', 'car'): 3.04,\n ('precedent', 'group'): 1.77,\n ('tiger', 'cat'): 7.35,\n ('situation', 'isolation'): 3.88,\n ('dividend', 'payment'): 7.63,\n ('bird', 'cock'): 7.1,\n ('announcement', 'news'): 7.56,\n ('century', 'nation'): 3.16,\n ('cemetery', 'woodland'): 2.08,\n ('cup', 'article'): 2.4,\n ('fuck', 'sex'): 9.44,\n ('street', 'block'): 6.88,\n ('tiger', 'mammal'): 6.85,\n ('peace', 'insurance'): 2.94,\n ('smart', 'student'): 4.62,\n ('seafood', 'lobster'): 8.7,\n ('Harvard', 'Yale'): 8.13,\n ('architecture', 'century'): 3.78,\n ('peace', 'plan'): 4.75,\n ('stock', 'phone'): 1.62,\n ('president', 'medal'): 3.0,\n ('money', 'cash'): 9.15,\n ('morality', 'importance'): 3.31,\n ('Japanese', 'American'): 6.5,\n ('Arafat', 'Jackson'): 2.5,\n ('month', 'hotel'): 1.81,\n ('life', 'term'): 4.5,\n ('money', 'dollar'): 8.42,\n ('smart', 'stupid'): 5.81,\n ('lad', 'brother'): 4.46,\n ('furnace', 'stove'): 8.79,\n ('tiger', 'jaguar'): 8.0,\n ('coast', 'forest'): 3.15,\n ('man', 'woman'): 8.3,\n ('vodka', 'brandy'): 8.13,\n ('forest', 'graveyard'): 1.85,\n ('drink', 'ear'): 1.31,\n ('doctor', 'personnel'): 5.0,\n ('theater', 'history'): 3.91,\n ('bishop', 'rabbi'): 6.69,\n ('car', 'flight'): 4.94,\n ('stock', 'life'): 0.92,\n ('money', 'currency'): 9.04,\n ('ministry', 'culture'): 4.69,\n ('dollar', 'buck'): 9.22,\n ('start', 'year'): 4.06,\n ('focus', 'life'): 4.06,\n ('street', 'place'): 6.44,\n ('cup', 'entity'): 2.15,\n ('century', 'year'): 7.59,\n ('image', 'surface'): 4.56,\n ('cup', 'artifact'): 2.92,\n ('chord', 'smile'): 0.54,\n ('mile', 'kilometer'): 8.66,\n ('listing', 'proximity'): 2.56,\n ('deployment', 'departure'): 4.25,\n ('man', 'governor'): 5.25,\n ('jaguar', 'car'): 7.27,\n ('glass', 'metal'): 5.56,\n ('food', 'fruit'): 7.52,\n ('direction', 'combination'): 2.25,\n ('bread', 'butter'): 6.19,\n ('profit', 'loss'): 7.63,\n ('production', 'hike'): 1.75,\n ('football', 'soccer'): 9.03,\n ('consumer', 'energy'): 4.75,\n ('midday', 'noon'): 9.29,\n ('school', 'center'): 3.44,\n ('possibility', 'girl'): 1.94,\n ('practice', 'institution'): 3.19,\n ('media', 'radio'): 7.42,\n ('magician', 'wizard'): 9.02,\n ('liquid', 'water'): 7.89,\n ('consumer', 'confidence'): 4.13,\n ('king', 'rook'): 5.92,\n ('boy', 'lad'): 8.83,\n ('car', 'automobile'): 8.94,\n ('precedent', 'collection'): 2.5,\n ('tiger', 'feline'): 8.0,\n ('shore', 'woodland'): 3.08,\n ('delay', 'racism'): 1.19,\n ('shower', 'thunderstorm'): 6.31,\n ('morality', 'marriage'): 3.69,\n ('journey', 'voyage'): 9.29,\n ('jaguar', 'cat'): 7.42,\n ('media', 'gain'): 2.88,\n ('street', 'children'): 4.94,\n ('psychology', 'psychiatry'): 8.08,\n ('stock', 'CD'): 1.31,\n ('lobster', 'wine'): 5.7,\n ('physics', 'chemistry'): 7.35,\n ('marathon', 'sprint'): 7.47,\n ('wood', 'forest'): 7.73,\n ('cup', 'tableware'): 6.85,\n ('cucumber', 'potato'): 5.92,\n ('coast', 'hill'): 4.38,\n ('travel', 'activity'): 5.0,\n ('murder', 'manslaughter'): 8.53,\n ('gem', 'jewel'): 8.96,\n ('dollar', 'yen'): 7.78,\n ('sugar', 'approach'): 0.88,\n ('planet', 'star'): 8.45,\n ('asylum', 'madhouse'): 8.87,\n ('calculation', 'computation'): 8.44,\n ('cell', 'phone'): 7.81,\n ('professor', 'doctor'): 6.62,\n ('seafood', 'food'): 8.34,\n ('television', 'radio'): 6.77,\n ('delay', 'news'): 3.31,\n ('championship', 'tournament'): 8.36,\n ('noon', 'string'): 0.54,\n ('doctor', 'nurse'): 7.0,\n ('rock', 'jazz'): 7.59,\n ('viewer', 'serial'): 2.97,\n ('volunteer', 'motto'): 2.56,\n ('experience', 'music'): 3.47,\n ('Mexico', 'Brazil'): 7.44,\n ('tiger', 'tiger'): 10.0,\n ('aluminum', 'metal'): 7.83,\n ('report', 'gain'): 3.63,\n ('attempt', 'peace'): 4.25,\n ('chance', 'credibility'): 3.88,\n ('street', 'avenue'): 8.88,\n ('announcement', 'effort'): 2.75,\n ('Mars', 'water'): 2.94,\n ('monk', 'oracle'): 5.0,\n ('cup', 'substance'): 1.92,\n ('student', 'professor'): 6.81,\n ('football', 'tennis'): 6.63,\n ('train', 'car'): 6.31,\n ('food', 'rooster'): 4.42,\n ('rooster', 'voyage'): 0.62,\n ('crane', 'implement'): 2.69,\n ('problem', 'airport'): 2.38,\n ('development', 'issue'): 3.97,\n ('tiger', 'organism'): 4.77,\n ('king', 'queen'): 8.58,\n ('profit', 'warning'): 3.88,\n ('type', 'kind'): 8.97,\n ('energy', 'secretary'): 1.81,\n ('planet', 'moon'): 8.08,\n ('lobster', 'food'): 7.81,\n ('precedent', 'information'): 3.85,\n ('benchmark', 'index'): 4.25}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_map = {(record[0],record[1]): record[2] for record in data.values.tolist()}\n",
    "score_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_sTFACx3dAk8"
   },
   "source": [
    "#with open(\"wordsim_similarity_goldstandard.txt\", encoding=\"utf-8\") as rf:\n",
    "#  triples = [line.strip().split(\"\\t\") for line in rf.readlines()]\n",
    "#  score_map = {tuple(triple[:2]): float(triple[2]) for triple in triples}"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRm9S90NBUAs"
   },
   "source": [
    "Note, that we took just expert similarity estimates from the original file and for nouns only. The original set is available [here](http://alfonseca.org/pubs/ws353simrel.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96B0OtKrvtaG"
   },
   "source": [
    "Let's have a look at similarity measure examples. \n",
    "\n",
    "Some words can have several different meanings in WordNet. Here -- just as an example -- we will select the first one that comes across, but then we will work with them differently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "7iXamIiZgf-O",
    "outputId": "c5b3c7ea-c476-4235-f474-b1358c584ddf"
   },
   "source": [
    "for w1, w2 in list(score_map)[:2]:\n",
    "  \n",
    "  print(\"\\nWords: %s-%s\\nGround truth score: %.2f\" % (w1, w2, score_map[(w1, w2)]))\n",
    "  \n",
    "  ss1 = wn.synset(w1 + \".n.01\")\n",
    "  ss2 = wn.synset(w2 + \".n.01\")\n",
    "\n",
    "  print(\"\\nPath: %.3f\" % ss1.path_similarity(ss2), end=\" \")\n",
    "  print(\"\\nwup: %.3f\" % ss1.wup_similarity(ss2), end=\" \")\n",
    "  print(\"\\nshortest_path: %.3f\" % ss1.shortest_path_distance(ss2))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words: professor-cucumber\n",
      "Ground truth score: 0.31\n",
      "\n",
      "Path: 0.077 \n",
      "wup: 0.500 \n",
      "shortest_path: 12.000\n",
      "\n",
      "Words: monk-slave\n",
      "Ground truth score: 0.92\n",
      "\n",
      "Path: 0.200 \n",
      "wup: 0.667 \n",
      "shortest_path: 4.000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHM7tCb0vqNp"
   },
   "source": [
    "Compute several similarity measures for all word pairs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe7Nuglqgnd3"
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "list_pairs = list(score_map)\n",
    "wup_list, true_list, path_list, lch_list = [], [], [], []\n",
    "\n",
    "# для всех пар\n",
    "for w1, w2 in list_pairs:\n",
    "\n",
    "  try:\n",
    "    all_w1 = wn.synsets(w1, pos=\"n\")\n",
    "    all_w2 = wn.synsets(w2, pos=\"n\")\n",
    "\n",
    "    # we add metrics of interest and expert reviews\n",
    "    wup = max([item1.wup_similarity(item2) \\\n",
    "                for item1, item2 in product(all_w1, all_w2)])\n",
    "    wup_list.append(wup)\n",
    "\n",
    "    path = max([item1.path_similarity(item2) \\\n",
    "                for item1, item2 in product(all_w1, all_w2)])\n",
    "    path_list.append(path)\n",
    "\n",
    "    lch = max([item1.lch_similarity(item2) \\\n",
    "                for item1, item2 in product(all_w1, all_w2)])\n",
    "    lch_list.append(lch)\n",
    "\n",
    "    true_list.append(score_map[(w1, w2)])\n",
    "\n",
    "  except Exception as e:\n",
    "    print(w1, w2, \"error:\", e)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAjuTLB0fD-I"
   },
   "source": [
    "## Calculate Spearman's rank correlation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "fXnCdw8wxcVd",
    "outputId": "c9e2b06c-5a27-45e8-a400-36113a3d7516"
   },
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "coef, p = spearmanr(wup_list, true_list)\n",
    "print(\"wup  Spearman R: %.4f\" % coef)\n",
    "\n",
    "coef, p = spearmanr(path_list, true_list)\n",
    "print(\"path Spearman R: %.4f\" % coef)\n",
    "\n",
    "coef, p = spearmanr(lch_list, true_list)\n",
    "print(\"lch Spearman R: %.4f\" % coef)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wup  Spearman R: 0.6936\n",
      "path Spearman R: 0.6535\n",
      "дср Spearman R: 0.6535\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "91"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wn.synsets(\"wood\", pos=\"n\")[0].hyponyms())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('alder.n.01'),\n Synset('ash.n.03'),\n Synset('balsa.n.01'),\n Synset('bamboo.n.01'),\n Synset('basswood.n.01'),\n Synset('beech.n.02'),\n Synset('beefwood.n.02'),\n Synset('bentwood.n.01'),\n Synset('birch.n.01'),\n Synset('black_locust.n.01'),\n Synset('blackwood.n.01'),\n Synset('boxwood.n.01'),\n Synset('brazilwood.n.01'),\n Synset('briarwood.n.01'),\n Synset('brushwood.n.01'),\n Synset('burl.n.01'),\n Synset('cabinet_wood.n.01'),\n Synset('cedar.n.02'),\n Synset('cherry.n.01'),\n Synset('chestnut.n.01'),\n Synset('citronwood.n.01'),\n Synset('cocuswood.n.01'),\n Synset('cypress.n.01'),\n Synset('dogwood.n.02'),\n Synset('driftwood.n.01'),\n Synset('dyewood.n.01'),\n Synset('ebony.n.02'),\n Synset('elm.n.02'),\n Synset('eucalyptus.n.01'),\n Synset('fir.n.01'),\n Synset('fruitwood.n.01'),\n Synset('granadilla_wood.n.01'),\n Synset('guaiac_wood.n.01'),\n Synset('gumwood.n.01'),\n Synset('hardwood.n.01'),\n Synset('hazel.n.02'),\n Synset('heartwood.n.01'),\n Synset('hemlock.n.03'),\n Synset('hickory.n.01'),\n Synset('incense_wood.n.01'),\n Synset('ironwood.n.02'),\n Synset('kauri.n.03'),\n Synset('kingwood.n.01'),\n Synset('knot.n.03'),\n Synset('lancewood.n.01'),\n Synset('larch.n.01'),\n Synset('lemonwood.n.01'),\n Synset('lignum_vitae.n.01'),\n Synset('locust.n.02'),\n Synset('log.n.01'),\n Synset('logwood.n.01'),\n Synset('mahogany.n.01'),\n Synset('maple.n.01'),\n Synset('matchwood.n.01'),\n Synset('matchwood.n.02'),\n Synset('oak.n.01'),\n Synset('obeche.n.01'),\n Synset('olive.n.03'),\n Synset('orangewood.n.01'),\n Synset('panama_redwood.n.01'),\n Synset('pecan.n.01'),\n Synset('pine.n.02'),\n Synset('poon.n.01'),\n Synset('poplar.n.01'),\n Synset('pyinma.n.01'),\n Synset('raw_wood.n.01'),\n Synset('red_lauan.n.01'),\n Synset('redwood.n.01'),\n Synset('rosewood.n.01'),\n Synset('ruby_wood.n.01'),\n Synset('sabicu.n.01'),\n Synset('sandalwood.n.01'),\n Synset('sandarac.n.01'),\n Synset('sapwood.n.01'),\n Synset('satinwood.n.02'),\n Synset('sawdust.n.01'),\n Synset('shittimwood.n.03'),\n Synset('silver_quandong.n.01'),\n Synset('softwood.n.01'),\n Synset('spruce.n.01'),\n Synset('sumac.n.01'),\n Synset('sycamore.n.01'),\n Synset('teak.n.01'),\n Synset('tulipwood.n.01'),\n Synset('tulipwood.n.02'),\n Synset('tupelo.n.01'),\n Synset('walnut.n.02'),\n Synset('wicker.n.01'),\n Synset('yellowwood.n.01'),\n Synset('yew.n.01'),\n Synset('zebrawood.n.01')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"wood\", pos=\"n\")[0].hyponyms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}