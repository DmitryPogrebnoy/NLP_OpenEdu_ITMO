{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "colab": {
   "name": "3_Students_1_en.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAyzWDzFdnLa"
   },
   "source": [
    "## Text classification: Spam or Ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh91cKPmdnLb"
   },
   "source": [
    "In this example based on the classical dataset Spambase Dataset (https://archive.ics.uci.edu/ml/datasets/spambase) we will try to make our own spam filter using scikit-learn library. The dataset contains text corpora of  5.574 text messages with labels \"spam\" or \"ham\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4Bf-AmgdnLb"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWvXDXKrdnLb"
   },
   "source": [
    "Data are attached to the task description for your convinience"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UnJwvQzbdnLb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "outputId": "6acd9477-a7f3-4006-9b58-9bebaa6997bc"
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('3_data.csv', encoding='latin-1')"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRqzedIIdnLc"
   },
   "source": [
    "We delete all other columns except for two of interest: text messages and labels:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xO59-HEadnLc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "outputId": "a76cadef-29e4-445c-81a6-cf0cee082b49"
   },
   "source": [
    "df = df[['v1', 'v2']]\n",
    "df = df.rename(columns = {'v1': 'label', 'v2': 'text'})\n",
    "df.head()"
   ],
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "  label                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU2xwmvIdnLd"
   },
   "source": [
    "Delete duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iveCG_bXdnLd"
   },
   "source": [
    "df = df.drop_duplicates('text')"
   ],
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuPip3mzdnLd"
   },
   "source": [
    "Change labels to binary:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JsKdfy6-dnLd"
   },
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vQJK8LNdnLe"
   },
   "source": [
    "### Text pre-processing (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8tefVdTdnLe"
   },
   "source": [
    "We need to complete the function for text pre-processing, to pre-process the text the following way:\n",
    "* convert text to lowercase;\n",
    "* remove stop-words;\n",
    "* remove punctuation marks;\n",
    "* normalizes the text using Snowball stemmer.\n",
    "\n",
    "We recommend to use the NLTK library, in order not to compile a list of stop-words and not to implement the stemming algorithm yourself. Click the link to find the examples of stemmers application (https://www.nltk.org/howto/stem.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yhU1BvAHdnLe"
   },
   "source": [
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Ваш код здесь\n",
    "    text = text.split(sep=\" \")\n",
    "    text = [word.lower() for word in text if not word in stopwords]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \" \".join(text)"
   ],
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM-Lrt6ddnLe"
   },
   "source": [
    "Check that the function works correctly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RSuPqUb2dnLe"
   },
   "source": [
    "assert preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\") == \"im gonna home soon dont want talk stuff anymor tonight k ive cri enough today\"\n",
    "assert preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\") == \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\""
   ],
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84H8mOmpdnLe"
   },
   "source": [
    "Apply to the text:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NtM4626ednLe"
   },
   "source": [
    "df['text'] = df['text'].apply(preprocess)"
   ],
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRyqtqUtdnLe"
   },
   "source": [
    "### Split the data to the training and test set (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nt7Z5NCMdnLe"
   },
   "source": [
    "y = df['label'].values"
   ],
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r4PJBctdnLe"
   },
   "source": [
    "Now we need to split the data to test (test) and training (train) sets. Scikit-learn library contains ready to use tools to do it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r1c9ARWIdnLe"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=41)"
   ],
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOV7Ub4ldnLe"
   },
   "source": [
    "### Classifier training (Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enAzNefqdnLe"
   },
   "source": [
    "We came to the classifier training now.\n",
    "\n",
    "First we extract features from the texts. It is strongly recommened to try several methods in order to check how each method influences the result (more information on defferent text representation methods you can find on the link https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
    "\n",
    "Then we train the classifier. We use SVM, but you can try different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QtfcmJ7NdnLe"
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# exctract features from the texts\n",
    "vectorizer = TfidfVectorizer(decode_error='ignore')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ],
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PIQQo8j3dnLe"
   },
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#train SVM model\n",
    "\n",
    "model = LinearSVC(random_state = 41, C = 1.1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ],
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn9kvQaZdnLe"
   },
   "source": [
    "Selfcheck. If the function ```preprocess``` is complimented correctly, then you should get the following model evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zGxDEYnjdnLe"
   },
   "source": [
    "print(classification_report(y_test, predictions, digits=3))"
   ],
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.979     0.996     0.987       898\n",
      "           1      0.967     0.860     0.911       136\n",
      "\n",
      "    accuracy                          0.978      1034\n",
      "   macro avg      0.973     0.928     0.949      1034\n",
      "weighted avg      0.978     0.978     0.977      1034\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nffLu6UdnLf"
   },
   "source": [
    "Let's predict results for the specified text"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "prWswDzudnLf"
   },
   "source": [
    "txt = \"Take your prize, more than 100 computers, smartphones and TVs are supposed to be played in a free quiz. Call by phone 8 800 243 456\"\n",
    "txt = preprocess(txt)\n",
    "txt = vectorizer.transform([txt])"
   ],
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "brfpnzR7dnLf"
   },
   "source": [
    "model.predict(txt)"
   ],
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1], dtype=int64)"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aVfQFiwGdnLf"
   },
   "source": [
    "The message is classified as spam."
   ],
   "execution_count": 73,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3549593910.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [73]\u001B[1;36m\u001B[0m\n\u001B[1;33m    The message is classified as spam.\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  }
 ]
}